# Dataset

- **LIP**:50462 images


# JPP-Net

**paper:**[Look into Person: Joint Body Parsing & Pose Estimation Network and A New Benchmark](https://arxiv.org/abs/1804.01984)`T-PAMI2018`

**code**:[LIP_JPPNet](https://github.com/Engineering-Course/LIP_JPPNet)

## Abstract

人类解析和姿态估计最近受到了相当大的关注。然而，现有数据集具有有限数量的图像和标注，并且缺乏无约束的环境中各种人类外观的具有挑战性的案例。在本文中，我们引入了一个名为“LIP”的新基准，它在可扩展性，多样性和难度方面都有重大进步。这个全面的数据集包含超过50,000个精心标注的图像，包含19个语义部分标签和16个身体关节，这些图像是从广泛的视角、遮挡和复杂背景中捕获的。使用这些丰富的标注，我们对人体解析和姿态估计方法进行详细分析，从而深入了解这些方法的成功与失败。为了进一步探索和利用这两个任务的语义相关性，我们提出了一种新的联合人体解析和姿态估计网络来进行有效的上下文建模，可以同时预测解析和姿势，具有极高的质量。此外，我们通过探索一种新颖的自我监督结构敏感学习方法来简化网络以解决人体解析，该方法将姿态结构强加到解析结果中而无需额外的监督。

## Contributions

这篇论文主要是两个贡献：LIP数据集与JPPNet网络。（论文说自己有三大贡献，还有一个贡献是探索多个前沿方法在不同数据集上的表现，分析Parsing与Pose的关系，并以此说明自己的数据集最具区分性，自己的方法最好）

LIP数据集是截止到2018年初最大的像素级别的人体区域解析和姿态估计的数据集，不仅有关节点的位置与连接，而且有人体区域的语义分割图，最重要的是，人体解析数据标注包含头发、眼镜、帽子

## Results

